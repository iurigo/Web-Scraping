{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Web Scraping and Spiders with `scrapy`\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Sam Stack(DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Learning Objectives\n",
    "- Understand the structure and content of HTML\n",
    "- Learn about elements, attributes, and element hierarchy in HTML\n",
    "- Learn about XPath and using multiple and singular selections\n",
    "- Practice using Scrapy to get data from craigslist\n",
    "- Practice using Beautiful Soup to parse data from craigslist\n",
    "- Walkthrough the construction of a spider built using scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypertext Markup Language\n",
    "\n",
    "Let's break that down further:\n",
    "\n",
    "\n",
    "> **Hypertext** - at its most basic level, is text which contains links to other texts and other forms of media such as image or video\n",
    "\n",
    "> **Markup Language** - a system for defining the presentation of text. HTML is a markup language for the web.\n",
    "\n",
    "> Kim Goulbourne, General Assembly\n",
    "\n",
    "HTML defines the **structure** of a webpage. This is important because as we'll see shortly webscraping is about identifying the structure of a webpage in a way that allows you to algorithmically target the portions of the page that hold the data you're after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the largest sources of data in the world is all around us.  We consume the web in some form every day.  One of the most powerful python toolsets we will learn allows us to extract and normalize data from unstructured sources like webpages.  \n",
    "\n",
    "**If you can see it, it can be scraped, mined, and put into a dataframe.**\n",
    "\n",
    "Before we begin the actual process of webscraping with python, it is important to cover the basic constructs that describe HTML as unstructured data. \n",
    "\n",
    "Then we will cover a powerful selection technique called XPath, and look at a basic workflow using a framework called [Scrapy](http://www.scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Document Object Model (The DOM)\n",
    "\n",
    "---\n",
    "\n",
    "\"The Document Object Model (DOM) provides a representation of the document as a structured group of nodes and objects that have properties and methods. With the DOM, programmers can build documents, navigate their structure, and add, modify, or delete elements and content. Anything found in an HTML document can be accessed, changed, deleted, or added using the DOM.\"\n",
    "\n",
    "- Kim Goulbourne, General Assembly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The DOM (continued)\n",
    "---\n",
    "In the HTML DOM (Document Object Model), everything is a node:\n",
    " * The document itself is a document node.\n",
    " * All HTML elements are element nodes.\n",
    " * All HTML attributes are attribute nodes.\n",
    " * Text inside HTML elements are text nodes.\n",
    " * Comments are comment nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elements\n",
    "Elements begin and end with open and close \"tags\", which are defined by namespaced, encapsulated strings. These namespaces that begin and end the elements must be the same.\n",
    "\n",
    "```\n",
    "<title>I am a title.</title>\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "\n",
    "As you may have several different titles or paragraphs on a single page, you can assign ID values to namespace to make more unique reference points.  IDs are also very useful for labelling nested elements.\n",
    "```\n",
    "<title id ='title_1'>I am a the first title.</title>\n",
    "<p id ='para_1'>I am the first paragraph.</p>\n",
    "<title id ='title_2'>I am a the second title.</title>\n",
    "<p id ='para_2'>I am the second paragraph.</p>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Elements can have parents and children:**\n",
    "It is important to remember that an element can be both a parent and a child and whether to refer to the element as a parent or a child depends on the specific element you are referencing.\n",
    "\n",
    "```\n",
    "<body id = 'parent'>\n",
    "    <div id = 'child_1'>I am the child of 'parent'\n",
    "        <div id = 'child_2'>I am the child of 'child_1'\n",
    "            <div id = 'child_3'>I am the child of 'child_2'\n",
    "                <div id = 'child_4'>I am the child of 'child_3'</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "**or**\n",
    "```\n",
    "<body id = 'parent'>\n",
    "    <div id = 'child_1'>I am the parent of 'child_2'\n",
    "        <div id = 'child_2'>I am the parent of 'child_3'\n",
    "            <div id = 'child_3'> I am the parent of 'child_4'\n",
    "                <div id = 'child_4'>I am not a parent </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attributes\n",
    "\n",
    "HTML elements can have attributes.  They describe properties, and characteristics of elements.  Some affect how the element behaves or looks in terms of the rendered output by the browser.\n",
    "\n",
    "Anchor elements have href attributes that tell the browser where to go after it is clicked.  Anchor elements are typically formatted in bold and underlined as a visual cue to differentiate itself.\n",
    "\n",
    "**Markup that describes an element with attributes, litterally looks like this:**\n",
    "\n",
    "```\n",
    "<a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">An Awesome Website</a>\n",
    "```\n",
    "\n",
    "**However, this element, once rendered, looks like this**\n",
    "\n",
    "[An Awesome Website](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Element hierarchy Visually Represented\n",
    "\n",
    "![Nodes](http://www.computerhope.com/jargon/d/dom1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Element hierarchy in code:\n",
    "\n",
    "```\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Example</title>\n",
    "    </head>\n",
    "    \n",
    "    <body>\n",
    "        <h1>Example Page</h1>\n",
    "        <p>This is an example page.</p>\n",
    "    </body> \n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You are now qualified HTML experts\n",
    "\n",
    "Your HTML learning can continue...\n",
    "\n",
    "Read all about the different elements supported amongst modern browsers:\n",
    " * [HTML5 Cheatsheet](http://websitesetup.org/html5-cheat-sheet/)\n",
    " * [Mozilla HTML Element Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    " * [HTML5 Visual Cheatsheet](http://www.unitedleather.biz/PDF/HTML5-Visual-Cheat-Sheet1.pdf)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is XPath?\n",
    "\n",
    "---\n",
    "<img src=\"assets/obama_wiki.png\" width=\"550\", height=\"300\">\n",
    "Xpath is a syntax that we'll use to target sections of an HTML/XML page. You can think of Xpath as a query language for HTML/XML.\n",
    "\n",
    "Understanding how to identify elements and attributes within HTML documents gives us the capability to write simple expressions that create structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XPath Helper?\n",
    "\n",
    "---\n",
    "\n",
    "To make this process easier to deal with, we will be using XPath helper, which is a Chrome add on.  It's not necessary, but highly recommended to help build XPath expressions.\n",
    "\n",
    "[XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en)\n",
    "\n",
    "XPath expressions can select elements, element attributes, and element text.  These selections can be either to a single item, or multiple items.  Generally, if you're not specific enough, you will end up selecting multiple elements.\n",
    "\n",
    "\n",
    "<a id='multiple-selections'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple selections\n",
    "\n",
    "***Multiple selections*** are useful for capturing search results, or any repeating element.  For instance, the _titles_ of an apartment listing search results from Craigslist:\n",
    "\n",
    "\n",
    "**URL**\n",
    "\n",
    "[http://sfbay.craigslist.org/search/sfc/apa](http://sfbay.craigslist.org/search/sfc/apa)\n",
    "\n",
    "\n",
    "**Example HTML Markup**\n",
    "```\n",
    "...\n",
    "<span class=\"pl\"> \n",
    "    <time datetime=\"2016-01-12 23:27\" title=\"Tue 12 Jan 11:27:35 PM\">Jan 12</time> \n",
    "    <a href=\"/sfc/apa/5400584579.html\" data-id=\"5400584579\" class=\"hdrlnk\">Welcome home to a sweetly renovated four bedroom one and a half bath</a> \n",
    "</span>\n",
    "...\n",
    "```\n",
    "\n",
    "**XPath - Multiple Titles** _copy this into the XPath Helper Query box_\n",
    "```\n",
    "//a[@class='result-title hdrlnk']\n",
    "```\n",
    "\n",
    "**Returns (Ad Titles)**\n",
    "```\n",
    "***New Remodeled two bedroom Apartment***\n",
    "WONDERFUL ONE BR APARTMENT HOME\n",
    "Beautiful 1bed/1bath Apartment in Russian Hill NO SECURITY DEPOSIT\n",
    "Knockout SF View|Green Oasis|Private Driveway|Furnished\n",
    "3BR/3BA Spacious, Beautiful SOMA Loft: 5 month lease\n",
    "Nob Hill Large Studio - Light, Quiet, Lovely Building\n",
    "etc...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular selections\n",
    "\n",
    "***Singular selections*** are necessary when you want to grab specific, unique text within elements.  Here's an example of a details page on Craigslist:\n",
    "\n",
    "> *Note: this example may be expired if you view it sometime after July 31st, 2017. Please replace this with a current craigslist listing!\n",
    "\n",
    "**URL**\n",
    "\n",
    "[https://sfbay.craigslist.org/sfc/apa/d/exquisite-level-contemporary/6244612708.html)\n",
    "\n",
    "**HTML Markup**\n",
    "\n",
    "```\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 6244612708</p>\n",
    "    <p class=\"postinginfo\" style=\"opacity: 1;\">posted: <time class=\"timeago\" datetime=\"2017-07-31T18:51:09-0700\" title=\"2017-07-31  6:51pm\">38 minutes ago</time></p>\n",
    "```\n",
    "\n",
    "**XPath - Single Item**\n",
    "\n",
    "```\n",
    "//p[@class='postinginfo'][2]/time\n",
    "```\n",
    "**Returns (Time of posting or age of Post)**\n",
    "```\n",
    "39 minutes ago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XPATH HELPER SHORTCUTS!\n",
    "\n",
    "- Hit Ctrl-Shift-X (or Command-Shift-X on OS X), or click the XPath Helper button in the toolbar, to open the XPath Helper console.\n",
    "- Hold down Shift as you mouse over elements on the page. The query box will continuously update to show the XPath query for the element below the mouse pointer, and the results box will show the results for the current query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Note On Specificity\n",
    "\n",
    "The concept of specificity is recurrent in front end web development and other applications where one must traverse HTML. When you write your Xpath queries, you should write them as specifically as possible to ensure you target the element you intend while also not writing them so specifically that you sacrifice generalizability or readability. \n",
    "\n",
    "---\n",
    "\n",
    "As an example, let's return to our multiple selection Xpath query to be used on http://sfbay.craigslist.org/search/sfc/apa\n",
    "\n",
    "\n",
    "\n",
    "``\n",
    "//a[@class='result-title hdrlnk'] \n",
    "``\n",
    "\n",
    "\n",
    "If our intention was to grab **only the first** instance of the anchor tag with the class 'result-title hdrlnk', we were not being specific enough to target this. Instead, we've grabbed every anchor element of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "#### Two alternatives that achieve our goal of grabbing the first anchor element might be:\n",
    "\n",
    "1) Specify the first anchor element that satisfies our query:\n",
    "\n",
    "``\n",
    "(//a[@class='result-title hdrlnk'])[1]\n",
    "``\n",
    "\n",
    "2) Specify the id, a unique identifier for the anchor element on this page:\n",
    "\n",
    "``\n",
    "//a[@data-id='6247436293']\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The following query, however, would be overly specific:\n",
    "\n",
    "``\n",
    "/html[@class='js canvas draggable fileAPI geolocation hashChange matchMedia picture pushState placeholder no-touchCapable transitions localStorage']/body[@class='search has-map en desktop grid has-map-view-button']/section[@id='page-top']/form[@id='searchform']/div[@id='sortable-results']/ul[@class='rows']/li[@class='result-row'][1]/p[@class='result-info']/a[@class='result-title hdrlnk']\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple  example using `scrapy` and `XPath`.\n",
    "\n",
    "---\n",
    "\n",
    "Below is an example of how to get information out of some fake HTML using the XPath capabilities of the `scrapy` package. You will likely need to install the scrapy package using `conda install scrapy`.   \n",
    "**Note:** `Conda install` will install the necessary dependent packages needed for Scrapy, `pip install` will **not**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the `Selector` class from the `Scrapy` library to help us construct our query.\n",
    "\n",
    "`Selector` classes take the HTML target as an argument and can then utilize several flavors of query type to extract information.  In our situation we will specify `XPath` as our query flavoured language (though we could also use CSS). \n",
    "\n",
    "Just like with writing python scripts, there are several ways you can access the exact same information in HTML.  Lets try a few out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "# HTML structure string\n",
    "HTML = \"\"\"\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span>\\\n",
    "    <span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Option 1: use the exact class name to get its associated text\n",
    "best = Selector(text=HTML).xpath(\"//span[@class='bestof-text']/text()\").extract()\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: use the 'contains()' function extract any text that includes the text 'best of'\n",
    "best = Selector(text=HTML).xpath(\"//span[contains(text(), 'best of')]/text()\").extract()\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 3: First grabs the entire html post where 'class='bestof-link'\n",
    "best =  Selector(text=HTML).xpath(\"/html/body/div/p/a[@class='bestof-link']\")\n",
    "# parse the first grabbed chunk for the the text of the specific element with class='bestof-text'\n",
    "nested_best =  best.xpath(\"./span[@class='bestof-text']/text()\").extract()\n",
    "nested_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Option 3 will probably be the most common for you because there is a good chance that you will want to grab information from several children elements that exist within one parent element._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where's Waldo - \"XPath Edition\"\n",
    "\n",
    "In this example, we will find Waldo together.  Find Waldo as:\n",
    "\n",
    "- Element\n",
    "- Attribute\n",
    "- Text element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "HTML = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        \n",
    "        <ul id=\"waldo\">\n",
    "            <li class=\"waldo\">\n",
    "                <span> yo Im not here</span>\n",
    "            </li>\n",
    "            <li class=\"waldo\">Height:  ???</li>\n",
    "            <li class=\"waldo\">Weight:  ???</li>\n",
    "            <li class=\"waldo\">Last Location:  ???</li>\n",
    "            <li class=\"nerds\">\n",
    "                <div class=\"alpha\">Bill gates</div>\n",
    "                <div class=\"alpha\">Zuckerberg</div>\n",
    "                <div class=\"beta\">Theil</div>\n",
    "                <div class=\"animal\">parker</div>\n",
    "            </li>\n",
    "        </ul>\n",
    "        \n",
    "        <ul id=\"tim\">\n",
    "            <li class=\"tdawg\">\n",
    "                <span>yo im here</span>\n",
    "            </li>\n",
    "        </ul>\n",
    "        <li>stuff</li>\n",
    "        <li>stuff2</li>\n",
    "        \n",
    "        <div id=\"cooldiv\">\n",
    "            <span class=\"dsi-rocks\">\n",
    "               YO!\n",
    "            </span>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <waldo>Waldo</waldo>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Tip:** We can use the asterisk special character '*' as an place holder for 'all possible'.\n",
    "\n",
    "```python\n",
    "# all elements where class='alpha'\n",
    "Selector(text=HTML).xpath('//*[@class=\"alpha\"]').extract()\n",
    "\n",
    "\n",
    "\n",
    "#returns\n",
    "\n",
    "[u'<div class=\"alpha\">Bill gates</div>',\n",
    " u'<div class=\"alpha\">Zuckerberg</div>']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<div class=\"alpha\">Bill gates</div>',\n",
       " u'<div class=\"alpha\">Zuckerberg</div>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=HTML).xpath('//*[@class=\"alpha\"]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Find element 'waldo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Waldo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text contents of the element waldo\n",
    "Selector(text=HTML).xpath('/html/body/waldo/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Find attribute(s) 'waldo'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<ul id=\"waldo\">\\n            <li class=\"waldo\">\\n                <span> yo Im not here</span>\\n            </li>\\n            <li class=\"waldo\">Height:  ???</li>\\n            <li class=\"waldo\">Weight:  ???</li>\\n            <li class=\"waldo\">Last Location:  ???</li>\\n            <li class=\"nerds\">\\n                <div class=\"alpha\">Bill gates</div>\\n                <div class=\"alpha\">Zuckerberg</div>\\n                <div class=\"beta\">Theil</div>\\n                <div class=\"animal\">parker</div>\\n            </li>\\n        </ul>',\n",
       " u'<li class=\"waldo\">\\n                <span> yo Im not here</span>\\n            </li>',\n",
       " u'<li class=\"waldo\">Height:  ???</li>',\n",
       " u'<li class=\"waldo\">Weight:  ???</li>',\n",
       " u'<li class=\"waldo\">Last Location:  ???</li>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of all attributes named waldo\n",
    "Selector(text=HTML).xpath('//*[@*=\"waldo\"]').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<li class=\"waldo\">\\n                <span> yo Im not here</span>\\n            </li>',\n",
       " u'<li class=\"waldo\">Height:  ???</li>',\n",
       " u'<li class=\"waldo\">Weight:  ???</li>',\n",
       " u'<li class=\"waldo\">Last Location:  ???</li>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of all class attributes named waldo\n",
    "Selector(text=HTML).xpath('//*[@class=\"waldo\"]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Find text element Waldo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<waldo>Waldo</waldo>']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets everything around the text element waldo\n",
    "Selector(text=HTML).xpath(\"//*[text()='Waldo']\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Requests + Beautiful Soup to extract information from a webpage.\n",
    "\n",
    "---\n",
    "\n",
    "Beautiful Soup is a python library useful for pulling data out of HTML and XML files.  It works with many parsers, such as XPath and can be executed in an IDE, so it can be much easier to work with when first extracting information from html.\n",
    "\n",
    "Please make sure that the required packages are installed: \n",
    "\n",
    "```bash\n",
    "# beautiful soup:\n",
    "> conda install bs4 \n",
    "> conda install lxml\n",
    "\n",
    "# or if conda doesn't work\n",
    "> pip install bs4\n",
    "> pip install lxml\n",
    "```\n",
    "\n",
    "Lets find another posting for a sweet set of wheels on Craigslist (You will probably have to update the URL to one that hasn't expired.):\n",
    "\n",
    "![](assets/craigslist.jpg)\n",
    "\n",
    "> *Note: you will need to update this to a current/working craigslist post.*\n",
    "\n",
    "https://washingtondc.craigslist.org/doc/cto/d/want-reliable-vehicle-for/6244528863.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: fetch the content by URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  200\n",
      "\n",
      "First part of HTML document fetched as string:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\">\n",
      "<head>\n",
      "<title>Want a Reliable Vehicle for Work? The 2010 Toyota Camry is Just the Ca - cars &amp; trucks - by owner - vehicle automotive sale</title>\n",
      "    \t<link rel=\"canonical\" href=\"http://washingtondc.craigslist.org/doc/cto/d/want-reliable-vehicle-for/6244528863.html\">\n",
      "\t<meta name=\"description\" content=\"Available to buy is our 10 Toyota Camry. AT. 4-door sedan has gotten just 97X00 ofmileage. Vehicle engine and wiring just maintained. Installed new brakes, \n"
     ]
    }
   ],
   "source": [
    "# you will need the requests library in order to fully utilize bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# target web page\n",
    "url = \"https://washingtondc.craigslist.org/doc/cto/d/want-reliable-vehicle-for/6244528863.html\"\n",
    "# establishing the connection to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "#Ex. 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found\n",
    "print 'Status Code: ',response.status_code\n",
    "\n",
    "# Pull HTML string out of requests and convert to a python string\n",
    "html = response.text\n",
    "\n",
    "# The first 500 characters of the content\n",
    "print \"\\nFirst part of HTML document fetched as string:\\n\"\n",
    "print html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[More information on request status codes](http://www.restapitutorial.com/httpstatuscodes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Parse HTML document with Beautiful Soup\n",
    "\n",
    "This step allows us to access the elements of the document by XPATH expressions. Soup queries are more like accessing information within a python object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note:** There are many ways to get the elements in a \"soup\" object\n",
    "\n",
    "Here are a few ways to select HMTL elements as \"objects\" within \"soup\" as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Want a Reliable Vehicle for Work? The 2010 Toyota Camry is Just the Ca - cars &amp; trucks - by owner - vehicle automotive sale</title>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# Grabbing the element\n",
    "soup.html.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want a Reliable Vehicle for Work? The 2010 Toyota Camry is Just the Ca - cars & trucks - by owner - vehicle automotive sale\n"
     ]
    }
   ],
   "source": [
    "# Just the text between elements\n",
    "print soup.html.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"header-logo\" href=\"/\" name=\"logoLink\">CL</a>,\n",
       " <a href=\"/\">washington, DC</a>,\n",
       " <a href=\"/doc/\">district of columbia</a>,\n",
       " <a href=\"/search/doc/sss\">for sale</a>,\n",
       " <a href=\"/search/doc/cto\">cars &amp; trucks - by owner</a>,\n",
       " <a href=\"https://post.craigslist.org/c/wdc\">post</a>,\n",
       " <a href=\"https://accounts.craigslist.org/login/home\">account</a>,\n",
       " <a class=\"favlink\" href=\"#\"><span aria-hidden=\"true\" class=\"icon icon-star fav\"></span><span class=\"fav-number\"></span><span class=\"fav-label\"> favorites</span></a>,\n",
       " <a class=\"to-banish-page-link\" href=\"#\">\\n<span aria-hidden=\"true\" class=\"icon icon-trash red\"></span>\\n<span class=\"banished_count\"></span>\\n<span class=\"discards-label\"> hidden</span>\\n</a>,\n",
       " <a class=\"header-logo\" href=\"/\">CL</a>,\n",
       " <a href=\"/reply/wdc/cto/6244528863\" id=\"replylink\">reply</a>,\n",
       " <a class=\"flaglink\" data-flag=\"28\" href=\"https://post.craigslist.org/flag?flagCode=28&amp;postingID=6244528863&amp;cat=cto&amp;area=wdc\" title=\"flag as prohibited / spam / miscategorized\"><span class=\"flag\">x</span> <span class=\"flagtext\">prohibited</span></a>,\n",
       " <a href=\"http://www.craigslist.org/about/prohibited\">?</a>,\n",
       " <a href=\"#\" id=\"printme\">print</a>,\n",
       " <a class=\"prev\">\\u25c0  prev </a>,\n",
       " <a class=\"backup\" title=\"back to search\">\\u25b2</a>,\n",
       " <a class=\"next\"> next \\u25b6 </a>,\n",
       " <a class=\"email-friend\" href=\"https://accounts.craigslist.org/eaf?postingID=6244528863&amp;token=U2FsdGVkX18yMDcwMTIwN1X7DlDdHXeIUMg-hTY35lVcZ8qxzGYHsTR4UwMxPs84L_lq6luyYWCXB-oWMofTsTizdCRaVrPePZWq1Rko1fE\">email to friend</a>,\n",
       " <a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=6244528863\" title=\"nominate for best-of-CL\">\\n<span class=\"bestof-icon\">\\u2665 </span><span class=\"bestof-text\">best of</span>\\n</a>,\n",
       " <a href=\"https://www.craigslist.org/about/best-of-craigslist\">?</a>,\n",
       " <a href=\"https://www.craigslist.org/about/safety\">safety tips</a>,\n",
       " <a href=\"https://www.craigslist.org/about/prohibited\">prohibited items</a>,\n",
       " <a href=\"https://www.craigslist.org/about/recalled_items\">product recalls</a>,\n",
       " <a href=\"https://www.craigslist.org/about/scams\">avoiding scams</a>,\n",
       " <a href=\"https://www.craigslist.org/about/scams\">Avoid scams, deal locally</a>,\n",
       " <a href=\"https://www.craigslist.org/about/help/\">help</a>,\n",
       " <a href=\"https://www.craigslist.org/about/scams\">safety</a>,\n",
       " <a href=\"https://www.craigslist.org/about/privacy.policy\">privacy</a>,\n",
       " <a href=\"https://forums.craigslist.org/?forumID=8\">feedback</a>,\n",
       " <a href=\"https://www.craigslist.org/about/craigslist_is_hiring\">cl jobs</a>,\n",
       " <a href=\"//www.craigslist.org/about/terms.of.use.en\">terms</a>,\n",
       " <a href=\"https://www.craigslist.org/about/\">about</a>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all anchor items in the soup\n",
    "soup.findAll(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"header-logo\" href=\"/\" name=\"logoLink\">CL</a>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find just the first anchor element in the soup\n",
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'CL'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find single or multiple elements\n",
    "# First parameter\n",
    "element = soup.findAll(\"a\", {\"class\": \"header-logo\"})\n",
    "element[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'$2050'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_search = soup.findAll('span', {\"class\": \"price\"})\n",
    "price_search[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# switching back to SF apartment listings\n",
    "response = requests.get(\"http://sfbay.craigslist.org/search/sfc/apa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "search_titles = soup.findAll(\"a\", {\"class\": \"hdrlnk\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data-id': '6253544409', 'href': '/sfc/apa/d/second-chance-3-br-gorgeous/6253544409.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6253669482', 'href': '/sfc/apa/d/luxury-spacious-studio/6253669482.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6213832532', 'href': '/sfc/apa/d/beautiful-3-bed-2-ba-gar-yard/6213832532.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6253615357', 'href': '/sfc/apa/d/stunning-views/6253615357.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6253657873', 'href': '/sfc/apa/d/love-purple-rain-the-beach/6253657873.html', 'class': ['result-title', 'hdrlnk']}\n"
     ]
    }
   ],
   "source": [
    "for link in search_titles[0:5]:\n",
    "    print link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### > **Check:** How do we know which parameters `findAll()` takes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice: can you select the price of our junker?  \n",
    "\n",
    " - Use XPath Helper to get an idea of where the element is within the HTML document.\n",
    " - Try to select using the soup.html.body.something.something method.\n",
    " - Try using findAll() to find a concise element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Spider with [Scrapy](http://scrapy.org/)\n",
    "\n",
    "---\n",
    "\n",
    "> *\"Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\"*\n",
    "\n",
    "Below we will walkthrough the creation of a **spider** using scrapy. Spiders are automated processes that will crawl through a webpage or webpages and collect information.\n",
    "\n",
    "> **Note:** This code should be written in a script outside of jupyter notebook.\n",
    "\n",
    "<a id='scrapy-project'></a>\n",
    "### 1. Create a new Scrapy project\n",
    "\n",
    "In your terminal. `cd` into a directory you want to create your Crawler's folder.  I recommend the desktop for ease of access to the files inside we will need to edit.\n",
    "> `scrapy startproject craigslist`\n",
    "\n",
    "**Should create output that looks like this:**\n",
    "<blockquote>\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Overridden settings: {}\n",
    "New Scrapy project 'craigslist' created in:\n",
    "    /Users/davidyerrington/virtualenvs/data/scraping/craigslist\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd craigslist\n",
    "    scrapy genspider example example.com\n",
    "</blockquote>\n",
    "\n",
    "**That command generates a set of project files:**\n",
    "<blockquote>\n",
    "craigslist/\n",
    "    scrapy.cfg\n",
    "    craigslist/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "</blockquote>\n",
    "\n",
    "Generally, these are our files.  We will go into more detail on these soon.\n",
    "\n",
    " * **`scrapy.cfg`:** the project configuration file\n",
    " * **`craigslist/`:** the project’s python module, you’ll later import your code from here.\n",
    " * **`craigslist/items.py`:** the project’s items file.\n",
    " * **`craigslist/pipelines.py`:** the project’s pipelines file.\n",
    " * **`craigslist/settings.py`:** the project’s settings file.\n",
    " * **`craigslist/spiders/`:** a directory where you’ll later put your spiders.\n",
    " \n",
    "Please add this line to your craigslist/settings.py file before continuing (under the 'NEWSPIDER_MODULE' is fine):\n",
    " \n",
    " <blockquote>\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " </blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Define an \"item\"\n",
    "\n",
    "Next, open the items.py file in your IDE so that we can define an item. \n",
    "\n",
    "When we define an item, it's telling our new application what it will be collecting.  In essence, an \"item\", is an entity that has attributes (ie: \"title\", \"description\", \"price\", etc) that are descriptive and relate to elements on pages that we will be scraping.  \n",
    "\n",
    "In more precise terms, this is a model (for those who are familliar with ORM or relational database terms).  Don't worry if this is a foreign concept.  The main idea to understand is that a model has attributes that closely resemble / relate to elements on our target web page(s).\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CraigslistItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. A spider that crawls\n",
    "\n",
    "An item is a model that resembles data on a webpage.  A spider is something that crawls pages and uses our item model to to get and hold items for us.\n",
    "\n",
    "**Scrapy spiders are python classes.  Let's write our first file, called `craigslist_spider.py` and put it in our `/spiders` directory:**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = \"craigslist\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "        \"http://sfbay.craigslist.org/search/sfc/apa\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "\n",
    "**Next, let's dive in and crawl from our `/craigslist/craigslist` directory:**\n",
    "\n",
    "```\n",
    "> scrapy crawl craigslist\n",
    "```\n",
    "\n",
    "**What just happened?**\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * Ran parse over the content containing the HTML markup, of each request URL.\n",
    " * What else?\n",
    " \n",
    "```python\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.body)\n",
    "```\n",
    "\n",
    "It saved a file in our base project directory.  It should be named based on the end of the URL.  In our case, it should create a file called \"sfc\".  This is taken directly from the Scrapy docs and it's only point is to illustrate the workflow so far.  It is kind of nice to have a reference to our HTML file though.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. XPath + parsing with our spider\n",
    "\n",
    "So far, we've defined what fields we'll get, some urls to fetch, and saved some content to a file.  Let's actually do something interesting.\n",
    "\n",
    "**We should let our spider know about the item model we made earlier.  In the head of the `craigslist/craigslist/spiders/craigslist_spider.py`, lets add a new import:**\n",
    "\n",
    "```python\n",
    "from craigslist.items import CraigslistItem\n",
    "```\n",
    "\n",
    "<br><br><br>\n",
    "**Let's replace our parse method, to find some data from our Craigslist spider response, and map it to our item model, CraigslistItem:**\n",
    "\n",
    "\n",
    "```python\n",
    "def parse(self, response): # define parse function \n",
    "    items = [] # element for storing scraped info\n",
    "\thxs = scrapy.Selector(response) # selector is a function that allows us to grab html from the response(target website)\n",
    "\tfor sel in hxs.xpath(\"//li[@class='result-row']/p\"): # as we're using xpath languange we need to specify\n",
    "                        # that the paragraphs we are trying to isolate are expressed via xpath\n",
    "\t\titem = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"a/text()\").extract() #title text from the 'a' element \n",
    "\t\titem['link']  =  sel.xpath(\"a/@href\").extract() # href/url from the 'a' element \n",
    "\t\titem['price'] =  sel.xpath('span/span[@class=\"result-price\"]/text()').extract()[0]\n",
    "                # price from the result price class nested in a few span elements.\n",
    "        items.append(item)\n",
    "\treturn items # shows scraped information as terminal output\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Save and examine our scraped data\n",
    "\n",
    "By default, we can save our crawled data as csv.  To save our data, we just need to pass a few optional parameters to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "> scrapy crawl craigslist -o items.csv -t csv\n",
    "</blockquote>\n",
    "\n",
    "It's always good to iteratively check our data when developing a spider to make sure it's close to what we want. \n",
    "\n",
    "> *Pro tip:  The longer your iterations are between checks, the harder it's going to be to understand what's not working and fix bugs.*\n",
    "\n",
    "You should now have a file called '`items.csv`' in the directory you ran the `scrapy crawl` command from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Addendum: leveraging XPath to get more results\n",
    "\n",
    "---\n",
    "\n",
    "Generally, a workflow that is useful in this context is to load the page in your Chrome browser, check out the page using the XPath Helper plugin, and from that derive your own XPath expressions based on the output.\n",
    "\n",
    "`text()` selects only the text of a given element (between the tags), and `@attribute_name` is used to select attributes.\n",
    "\n",
    "**Here are a few examples of `text()`**:\n",
    "<blockquote>\n",
    "<h1>Darwin - The Evolution Of An Exhibition</h1>\n",
    "</blockquote>\n",
    "\n",
    "The XPath selector for this:\n",
    "\n",
    "<blockquote>\n",
    "//h1/text()\n",
    "</blockquote>\n",
    "\n",
    "**Here are a few examples of attributes**:\n",
    "\n",
    "And the description is contained inside a `<div>` tag with `id=\"description\"`:\n",
    "<blockquote>\n",
    "<h2>Description:</h2>\n",
    "\n",
    "<div id=\"description\">\n",
    "Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.\n",
    "</div>\n",
    "...\n",
    "</blockquote>\n",
    "\n",
    "XPath\n",
    "<blockquote>\n",
    "//div[@id='description']\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Following links for more results\n",
    "\n",
    "100 results is pretty cool but what if we want more?  We need to follow the \"next\" links, and find new pages to grab.  We're going to update the **`parse()`** method of our spider class, and use pandas to write the data to csv.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from craigslist.items import CraigslistItem\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = 'craigslist'\n",
    "    allowed_domains = ['craigslist.org']\n",
    "    start_urls = ['http://sfbay.craigslist.org/search/sfc/apa']\n",
    "\n",
    "    def parse(self, response):\n",
    "        items = []\n",
    "        hxs = scrapy.Selector(response)\n",
    "        titles = hxs.xpath(\"//li[@class='result-row']/p\")\n",
    "\n",
    "        for sel in titles:\n",
    "            item = CraigslistItem()\n",
    "            item['title'] = sel.xpath('a/text()').extract()\n",
    "            item['link'] = sel.xpath('a/@href').extract()\n",
    "            item['price'] = \\\n",
    "                sel.xpath('span/span[@class=\"result-price\"]/text()'\n",
    "                          ).extract()[0]\n",
    "            items.append(item)  # return items\n",
    "        df = pd.DataFrame(items)\n",
    "\n",
    "        with open('Mike.csv', 'a') as f:\n",
    "        \tdf.to_csv(f, header= False, index=False)\n",
    "\n",
    "        # Does the next page exist ? Let 's get it!\n",
    "\n",
    "        next_page = \\\n",
    "            response.xpath(\"(//a[@class='button next']/@href)[1]\")\n",
    "\n",
    "        if next_page:\n",
    "            url = response.urljoin(next_page[0].extract())\n",
    "            yield scrapy.Request(url, self.parse)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To call this command, enter: \n",
    "\n",
    "<blockquote>\n",
    "> scrapy crawl craigslist\n",
    "</blockquote>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
